{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5e58e022-b561-4fe1-bb38-d930eb69662c",
   "metadata": {},
   "source": [
    "### Notes\n",
    "- probably not enough data to use word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e9a4cdb9-1420-41f5-b08b-f01ad9a40b83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import collections\n",
    "from datetime import datetime as dt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import string\n",
    "from tqdm.auto import tqdm # For debugging\n",
    "\n",
    "# Custom imports\n",
    "from utils import accessors, preprocess, validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3236e466-47a4-4f94-a2d4-c6280bb77b1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(500)\n",
    "show_outputs = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b2e0ba7a-3fbb-4a50-8e32-1935228dffe9",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'accessoars' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [3]\u001b[0m, in \u001b[0;36m<cell line: 7>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m cit_file \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcities.jsonl\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# Load the datasets in pandas\u001b[39;00m\n\u001b[0;32m----> 7\u001b[0m address_df \u001b[38;5;241m=\u001b[39m \u001b[43maccessoars\u001b[49m\u001b[38;5;241m.\u001b[39mload_data(path, add_file) \n\u001b[1;32m      8\u001b[0m cit_df \u001b[38;5;241m=\u001b[39m accessors\u001b[38;5;241m.\u001b[39mload_data(path, cit_file) \n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m# Preprocess the datasets (remove punctuation and parse to lowercase)\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'accessoars' is not defined"
     ]
    }
   ],
   "source": [
    "# File and location information\n",
    "path = \"dataset/\"\n",
    "add_file = \"addresses.jsonl\"\n",
    "cit_file = \"cities.jsonl\"\n",
    "\n",
    "# Load the datasets in pandas\n",
    "address_df = accessoars.load_data(path, add_file) \n",
    "cit_df = accessors.load_data(path, cit_file) \n",
    "\n",
    "# Preprocess the datasets (remove punctuation and parse to lowercase)\n",
    "address_df = preprocess.clean_strings(address_df, 'address') \n",
    "address_df = address_df.drop_duplicates()\n",
    "cit_df = preprocess.clean_strings(cit_df, 'city') \n",
    "cit_df = cit_df.drop_duplicates()\n",
    "\n",
    "if show_outputs: \n",
    "    display(address_df)\n",
    "    display(cit_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f097c30-83f4-4442-a42e-8676dbb68ff8",
   "metadata": {},
   "source": [
    "### Quick data analytics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "979bb373-7872-4583-b012-28968fef515b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Any nans in the dataset?\n",
    "print(address_df.isna().sum())\n",
    "print(cit_df.isna().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26c2d0d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of occurences of each country \n",
    "countries = address_df.groupby('country').count()\n",
    "countries.plot.barh()\n",
    "countries.plot.barh(log = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dae1e2d6-678f-479a-af6c-5149ea2a3e0e",
   "metadata": {},
   "source": [
    "# Simple solution \n",
    "Split address into tokens and brute force search in city dataframe "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d19498f2-ccce-4dcd-8b64-53b2b350de00",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "t1 = dt.now()\n",
    "add_df = address_df.copy(deep=True).sample(n = 10000)\n",
    "# Split the address into a list\n",
    "add_df['address_split'] = add_df['address'].str.split()\n",
    "\n",
    "dupe_cities = collections.defaultdict(list)\n",
    "\n",
    "# Loop over each row, split address into tokens, and see if tokens are in city dataframe\n",
    "for i, row in tqdm(add_df.iterrows(), total=add_df.shape[0]): \n",
    "    address = row['address_split']\n",
    "    for token in address: \n",
    "        if token.isnumeric(): \n",
    "            pass\n",
    "        elif token in cit_df['city'].values:\n",
    "            try: \n",
    "                add_df.loc[i, 'country_out'] = cit_df.loc[cit_df['city'] == token, 'country'].item()\n",
    "            except: \n",
    "                dupe_cities[token].append(cit_df.loc[cit_df['city'] == token, 'country'].values)\n",
    "            \n",
    "display(add_df)\n",
    "\n",
    "t2 = dt.now()\n",
    "print(\"Time for calculation: {}\".format((t2-t1)))\n",
    "\n",
    "results = validation.accuracy(add_df, label_col = 'country', prediction_col = 'country_out')\n",
    "display(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0308cd7d-2118-436b-a1bb-cb0947f81991",
   "metadata": {},
   "source": [
    "### Short-comings\n",
    "- only really using the city information, and not any of the other address data\n",
    "- not all cities are one single string \n",
    "- special characters might change (e.g. ÃŸ -> ss) so the city will be missed\n",
    "- some cities exist in multiple countries\n",
    "- slow and not scalable"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac68bad5",
   "metadata": {},
   "source": [
    "# Simple ML solution \n",
    "Split address into tokens and brute force search in city dataframe \n",
    "https://medium.com/@bedigunjit/simple-guide-to-text-classification-nlp-using-svm-and-naive-bayes-with-python-421db3a72d34"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a486cfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from nltk import pos_tag\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from collections import defaultdict\n",
    "import sklearn \n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a65d5a94",
   "metadata": {},
   "source": [
    "## Train test split \n",
    "- Train on 80% of the data and use the remaining 20% to validate the approach\n",
    "- Stratify over country since they are not all equally occurring in the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f81df67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train test split\n",
    "train_x, test_x, train_y, test_y = sklearn.model_selection.train_test_split(address_df['address'],\n",
    "                                                                            address_df['country'],\n",
    "                                                                            stratify=address_df['country'],\n",
    "                                                                            test_size=0.2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "688a89ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine the city dataset with the address dataset for training\n",
    "train_x = pd.concat([train_x,cit_df['city']])\n",
    "train_y = pd.concat([train_y,cit_df['country']])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc0ac5d6",
   "metadata": {},
   "source": [
    "## Feature Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8565409",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialise label encoder (assigns numerical value to country str)\n",
    "Encoder = LabelEncoder()\n",
    "\n",
    "# Encode the labels\n",
    "train_y = Encoder.fit_transform(train_y)\n",
    "test_y = Encoder.fit_transform(test_y)\n",
    "\n",
    "if show_outputs: \n",
    "    print(train_y)\n",
    "    print(test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38a226b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialise term-frequency - inverse document frequency vectoriser\n",
    "tfidf_vect = TfidfVectorizer(max_features=5000)\n",
    "\n",
    "# Fit and transform on addresses\n",
    "tfidf_vect.fit(address_df['address'])\n",
    "train_x_tfidf = tfidf_vect.transform(train_x)\n",
    "test_x_tfidf = tfidf_vect.transform(test_x)\n",
    "\n",
    "if show_outputs: \n",
    "    print(train_x_tfidf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16918212",
   "metadata": {},
   "source": [
    "## Naive Bayes model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "245637f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit the training dataset on the NB classifier\n",
    "Naive = sklearn.naive_bayes.MultinomialNB()\n",
    "print(\"fitting...\")\n",
    "Naive.fit(Train_X_Tfidf,Train_Y)\n",
    "# predict the labels on validation dataset\n",
    "print(\"predictiong...\")\n",
    "predictions_NB = Naive.predict(Test_X_Tfidf)\n",
    "# Use accuracy_score function to get the accuracy\n",
    "print(\"Naive Bayes Accuracy: \", accuracy_score(predictions_NB, Test_Y))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebc60cee",
   "metadata": {},
   "source": [
    "### Notes on this approach \n",
    "- Very fast so scalable"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c5164bd",
   "metadata": {},
   "source": [
    "## Support Vector Machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bc1e853",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit the training dataset on the classifier\n",
    "SVM = sklearn.svm.SVC(C=1.0, kernel='linear', degree=3, gamma='auto')\n",
    "print(\"fitting...\")\n",
    "SVM.fit(Train_X_Tfidf,Train_Y)\n",
    "# predict the labels on validation dataset\n",
    "print(\"predicting...\")\n",
    "predictions_SVM = SVM.predict(Test_X_Tfidf)\n",
    "# Use accuracy_score function to get the accuracy\n",
    "print(\"SVM Accuracy: \",accuracy_score(predictions_SVM, Test_Y))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65bbdfbe",
   "metadata": {},
   "source": [
    "## Decision tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "004e53f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "tree = DecisionTreeClassifier()\n",
    "print(\"fitting...\")\n",
    "tree.fit(Train_X_Tfidf, Train_Y)\n",
    "# predict the labels on validation dataset\n",
    "print(\"predictiong...\")\n",
    "predictions_tree = tree.predict(Test_X_Tfidf)\n",
    "# Use accuracy_score function to get the accuracy\n",
    "print(\"Decision Tree Accuracy: \", accuracy_score(predictions_tree, Test_Y))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c0dd3ec",
   "metadata": {},
   "source": [
    "### Notes on this approach \n",
    "- Takes a while to fit \n",
    "- SVMs do not perform well when target classes overlap \n",
    "- Not particularly scalable as it's slow to fit and to predict on large datasets\n",
    "- The results are not significantly better than the simple Naive Bayes approach (92.83 NB vs 93.01 SVM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb31d956",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c823c1c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator\n",
    "class ClfSwitcher(BaseEstimator):\n",
    "\n",
    "    def __init__(self, estimator = SGDClassifier()):\n",
    "        \"\"\"\n",
    "        A Custom BaseEstimator that can switch between classifiers.\n",
    "        :param estimator: sklearn object - The classifier\n",
    "        \"\"\" \n",
    "\n",
    "        self.estimator = estimator\n",
    "\n",
    "\n",
    "    def fit(self, X, y=None, **kwargs):\n",
    "        self.estimator.fit(X, y)\n",
    "        return self\n",
    "\n",
    "\n",
    "    def predict(self, X, y=None):\n",
    "        return self.estimator.predict(X)\n",
    "\n",
    "\n",
    "    def predict_proba(self, X):\n",
    "        return self.estimator.predict_proba(X)\n",
    "\n",
    "\n",
    "    def score(self, X, y):\n",
    "        return self.estimator.score(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ca0289e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pipeline = Pipeline([('tfidf', TfidfVectorizer()), ('clf', ClfSwitcher()),])\n",
    "\n",
    "parameters = [\n",
    "    {\n",
    "        'clf__estimator': [SGDClassifier()], # SVM if hinge loss / logreg if log loss\n",
    "        'tfidf__max_df': (0.25, 0.5, 0.75, 1.0),\n",
    "        'tfidf__stop_words': ['english', None],\n",
    "        'clf__estimator__penalty': ('l2', 'elasticnet', 'l1'),\n",
    "        'clf__estimator__max_iter': [50, 80],\n",
    "        'clf__estimator__tol': [1e-4],\n",
    "        'clf__estimator__loss': ['hinge', 'log_loss', 'modified_huber'],\n",
    "    },\n",
    "    {\n",
    "        'clf__estimator': [MultinomialNB()],\n",
    "        'tfidf__max_df': (0.25, 0.5, 0.75, 1.0),\n",
    "        'tfidf__stop_words': ['english', None],\n",
    "        'clf__estimator__alpha': (1e-2, 1e-3, 1e-1),\n",
    "    },\n",
    "]\n",
    "\n",
    "gscv = GridSearchCV(pipeline, parameters, cv=5, n_jobs=12, return_train_score=False, verbose=3)\n",
    "gscv.fit(train_x, train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "450b758b",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_params = gscv.best_params_\n",
    "print(best_params)\n",
    "best_pipe = gscv.best_estimator_\n",
    "print(best_pipe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69102978",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Training set score: ' + str(gscv.score(train_x, train_y)))\n",
    "print('Test set score: ' + str(gscv.score(test_x, test_y)))\n",
    "\n",
    "# print('Training set accuracy: ' + str(gscv.accuracy(train_x, train_y)))\n",
    "# print('Test set accuracy: ' + str(gscv.accuracy(test_x, test_y)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14ccff7b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
